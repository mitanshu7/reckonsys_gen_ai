









PaperMatch Analysis




Home

PaperMatch Analysis
What do the vectors tell us?
Mitanshu Sukhwani
2025-06-11



3D Map of
arXiv

This post is heavily inspired by this blog post, Exploring
Hacker News by mapping and analyzing 40 million posts and comments for
fun, by Wilson
Lin.
In Behind PaperMatch, we saw a
brief overview of how to spin up a semantic search engine and in Behind PaperMatchBio we learned
about some hacks that go into scraping when you are low on resources.
But what about the data that these engines leave behind? Can we study
something deeper about the research that we do and how it evolves? Here
is my attempt at it.
Validation
My open-source model of choice, mixedbread-ai/mxbai-embed-large-v1,
was fairing at top spots on Massive Text
Embedding Benchmark (MTEB) leaderboard during the inception of PaperMatch (March 2024). Itâ€™s a small,
only 335 million parameters, embedding model compared to newer models
with billions of
parameters.
What do small models struggle with? Long context
windows. mxbai-embed-large-v1 only has a length of
\(512\) tokens. But how long are all
the abstracts on arXiv anyway?


Histogram of token lengths

On first look, bingo! most of the abstracts are covered by the
modelâ€™s context window. But how many are actually out of context? Enjoy
some stats below.
Total number of papers (as of June 2025): 2754926

Number of papers with token count > 512: 15548
Number of papers with token count <= 512: 2739378

Percentage of papers with token count > 512: 0.56%
Percentage of papers with token count <= 512: 99.44%

Minimum token count: 2
Average token count: 207.62
Maximum token count: 1595
Papers with the shortest abstract length:

Are
theoretical results â€˜Resultsâ€™?

Raymond E. Goldstein | July 2018

Yes.
Is AmI
(Attacks Meet Interpretability) Robust to Adversarial
Examples?

Nicholas Carlini | February 2019

No.

Paper with the longest abstract length:

On the
generation of Arveson weakly continuous semigroups

Jean Esterle (IMB) | September 2017

We consider here one-parameter semigroups
â€¦
we indeed have \(F(-A_T)=A_T.\)

Phew, model migrations are a pain.
Vector
Databases Are the Wrong Abstraction has some good
insights!
Performance
Distance Metric
Then I wanted to check and turn all the knobs I had. When you are
talking about semantic search, there are many metrics with which you can
define the â€œdistanceâ€ between two vectors.
The most common one is Euclidean.
Imagine two points on a plane with coordinates \(A := (x_1, y_1)\), and \(B := (x_2, y_2)\). The square of distance
between \(A\) and \(B\) is: \[
d(A,B)^2 = (y_2 - y_1 )^2 + (x_2 - x_1)^2 \] Euclidean distance
signifies how close (or far) two points are. Smaller the distance,
closer the points, and hence more related they are.
However, when talking about semantic search, the most used metric is
Cosine
Similarity.


Cosine similarity formula

Cosine similarity ranges from \(-1 \to
1\). Where \(-1\) tells you that
the vectors in question are facing opposite to each other, \(0\) tells you orthogonal vector and \(1\) tells you both the vectors are facing
in the same direction. Embedding models also capture semantic meaning in
terms of the direction. And hence, if two vectors are pointing in the
same direction, they tend to mean similar things.


Cosine similarity graph

Another such metric is Hamming
distance. It exists for binary vectors, values of which are either
\(0\) or \(1\). Hamming distance is defined by taking
bitwise XOR of
two binary vectors and then summing up the values in resulting
vector.


Hamming distance calulation

Fun fact, POPCNT -
counts the number of 1 bits, is actually The NSA
Instruction.
Index used by the Vector
Database
Milvus offers a range of distance metrics and vector indices. Weâ€™ll compare FLAT
and IVF_FLAT.
FLAT is a very simple index. It simply calculates the distance
between the query vector and all the vectors in your database. This
simplicity comes with the cost of having to compute a lot.
Inverted File FLAT (IVF-FLAT) allows for approximate nearest
neighbour search. First you partition your embedding database, then
calculate centroids of all the partitions.


Partition

Then, when a query vector comes, first you only query the centroids
and when you find the closest centroid, you use FLAT search for all the
vectors belonging to that cluster.


IVF-FLAT search

Source: How
to Pick a Vector Index in Your Milvus Instance: A Visual Guide.
One last thing we need to know about is how the search in PaperMatch works. The following
flowchart should make it clear.


How queries are handled in
PaperMatch.

Armed with the above parameters, I tried to estimate how they affect
the latency of the vector database.


Latency vs Search limit

We get the best quality when we use
COSINE similarity, Float32 vectors, along with
a FLAT index, and fastest results on
HAMMING distance, Binary vectors, and
IVF_FLAT index. Naturally, PaperMatch uses the latter.
But how much quality degradation do we really see? Well, not much
since the top ten results of both the variants are the
same. And assuming most people do not look beyond the top 10
results when then top ones werenâ€™t worthwhile in the first place, this
works out very well for user experience (UX).
What about the
direction of science itself?
Category
arXiv started in \(1991\) with only
about \(300\) physics papers in its
repo. Now it is one of the biggest preprint repositories hosting close
to \(3\) million articles and growing
at an average rate of about \(4.5K\)
papers/week. How have the subjects evolved?


2001 Categories Histogram



2011 Categories Histogram



2021 Categories Histogram



2025 Categories Histogram

Now you know why arXiv has so many CS papers. They account for
almost half of a yearâ€™s worth!
Dimensionality reduction
PCA
Principal
Component Analysis, is a statistical technique used to reduce the
dimensionality of data (from a vector of size \(1024\) to say \(2\)) by identifying the most important
features (principal components) that capture the most variance in a
dataset.


PCA in action

We can apply pca to float and binary vectors to see the impact of
conversion.
Float


Scatter plot PCA of float embeddings,
2011



Scatter plot PCA of float embeddings,
2021



Scatter plot PCA of float embeddings,
2025

Binary


Scatter plot PCA of binary embeddings,
2011



Scatter plot PCA of binary embeddings,
2021



Scatter plot PCA of binary embeddings,
2025

So that is why we get very good results in binary too!
However, there is another technique which gets itself a good name
owing to it quality of reductions.
UMAP
Uniform
Manifold Approximation and Projection is a novel manifold learning
technique for dimension reduction.
Float, Euclidean


Scatter plot UMAP of float embeddings,
2011



Scatter plot UMAP of float embeddings,
2021



Scatter plot UMAP of float embeddings,
2025

Binary, Hamming


Scatter plot UMAP of binary embeddings,
2011



Scatter plot UMAP of binary embeddings,
2021



Scatter plot UMAP of binary embeddings,
2025

UMAP seems to giving much better seperation between subjects. It is
quite interesting to see the â€œislandâ€ coming up for the
year 2025. The â€œislandâ€ originally appears in 2024. It
can be seen even when we use cosine distance.
Bonus! Mixedbread makes the
case for their model in the blog post: 64 bytes per
embedding, yee-haw ğŸ¤ . Their embedding models is compatible with Matryoshka Representation
Learning (MRL) and Vector
Quantization. Essentially, the vectors still hold strong when you
convert float32 to binary (\(1\) if they are greater than \(0\) and to \(0\) if they are not) and then chop the
vectors in half (\(1024 \to 512\)) or
so.
MRL
We simply take the first two elements of the embedded vectors
directly.
Float


Scatter plot MRL of float embeddings,
2011



Scatter plot MRL of float embeddings,
2021



Scatter plot MRL of float embeddings,
2025

Binary


Scatter plot MRL of binary embeddings,
2011



Scatter plot MRL of binary embeddings,
2021



Scatter plot MRL of binary embeddings,
2025

Float seems to performs quite well, but the binary one conveys no
information whatsoever.
3D Map of
arXiv
For fun, I also performed UMAP for all of arXiv
on the Binary vectors using Hamming distance
as metric. Peak memory usage reached 165 GB of RAM on
AMD EPYC 8434P 48-Core Processor running
Ubuntu Server. Float with Cosine
exceeded the RAM+SWAP (256 + 200 GB) and hence could not be
performed.
Please explore it at: 3D Map of
arXiv. Note that the number of points (papers) displayed
are sampled to 1,00,000 for performance reasons.
The source code & the data for the analysis is available at mitanshu7/PaperMatch_Analysis
and for the 3D map at mitanshu7/arxiverse.
Back to Home


